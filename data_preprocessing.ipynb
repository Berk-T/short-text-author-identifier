{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from model.classifier import RNNClassifier\n",
    "from torchsummary import summary\n",
    "from dataset.chat_dataset import preprocess_data, ChatDataset, create_vocab\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and Processing Chat Data\n",
    "\n",
    "Chat data should be in the dataset folder and named \"_chat.txt\". First open the text file in VSCode and check if [U+200E] characters are present, if so remove all occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.getcwd())\n",
    "chat_dir = os.path.join(path, \"dataset\")\n",
    "sender_indices = preprocess_data(chat_dir)\n",
    "\n",
    "print(sender_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing Data and Creating Vocabulary\n",
    "\n",
    "Now that we have preprocessed the data we can create our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tokenized_data, lines = create_vocab(chat_dir, sender_indices, threshold=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    # the token that is not in vocab get assigned <unk>\n",
    "    indexed_data.append((indices, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    data = (lines[i], tokenized_data[i][0], indexed_data[i][0], indexed_data[i][1])\n",
    "    combined_data.append(data)\n",
    "\n",
    "\n",
    "dataset = ChatDataset(combined_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    assert isinstance(batch, list)\n",
    "    data = pad_sequence([b['data'] for b in batch])\n",
    "    lengths = torch.tensor([len(b['data']) for b in batch])\n",
    "    label = torch.stack([b['label'] for b in batch])\n",
    "    return {\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'lengths': lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, collate_fn=collate)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        sampler=valid_sampler, collate_fn=collate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create and Train Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_accuracy(model, data_loader):\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, x in enumerate(data_loader):\n",
    "        input = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        label = x['label'].to(device)\n",
    "        pred = model(input, lengths)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        corrects += torch.count_nonzero(torch.eq(pred, label))\n",
    "        total += label.numel()\n",
    "\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print('Step {} / {}'.format(i, len(data_loader)))\n",
    "\n",
    "    return corrects / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "model_save_path = os.path.join(path, \"model\", \"chat_model.p\")\n",
    "\n",
    "# model = torch.load(model_save_path)\n",
    "model = RNNClassifier(len(vocab), 100, 64, len(sender_indices.keys()), num_layers=1)\n",
    "\n",
    "# Move model to the device we are using\n",
    "model = model.to(device)\n",
    "gclip = 8\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, loss_func, sched=None, epochs=10):\n",
    "    model.train()\n",
    "    for epoch_id in range(epochs):\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                tepoch.set_description(f'Epoch {epoch_id + 1}')\n",
    "                data, labels, lengths = data['data'].to(\n",
    "                    device), data['label'].to(device), data['lengths'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data, lengths)\n",
    "                outputs = outputs.to(device)\n",
    "                loss = loss_func(outputs, labels)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), gclip)\n",
    "                optimizer.step()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "            if sched:\n",
    "                sched.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "\n",
    "    table = PrettyTable([\"Mod name\", \"Parameters Listed\"])\n",
    "    t_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        t_params += param\n",
    "    print(table)\n",
    "    print(f\"Sum of trained paramters: {t_params}\")\n",
    "    return t_params\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = ExponentialLR(optimizer, gamma=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, optimizer, train_loader, val_loader, loss_func, epochs=20, sched=sched)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"accuracy on test set: {}\".format(compute_accuracy(model, val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model, model_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.chat_dataset import tokenize\n",
    "from time import sleep\n",
    "model.eval()\n",
    "\n",
    "pred_indices = {value:key for (key, value) in sender_indices.items()}\n",
    "\n",
    "text = input(\"Enter text: \")\n",
    "tokens = tokenize(text.lower())\n",
    "indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "sequence = torch.tensor([indices]).permute(1,0).to(device)\n",
    "pred = model.predict(sequence)\n",
    "print(f'{pred_indices[pred.item()]}: {text}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c00457d27ce730ef0cdcf7f4aede23c3643b99c873f2bc6551ffd637bf1447c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
