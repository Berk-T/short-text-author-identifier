{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from model.classifier import RNNClassifier\n",
    "from torchsummary import summary\n",
    "from dataset.chat_dataset import preprocess_data, ChatDataset, create_vocab\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and Processing Chat Data\n",
    "\n",
    "Chat data should be in the dataset folder and named \"_chat.txt\". First open the text file in VSCode and check if [U+200E] characters are present, if so remove all occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.getcwd())\n",
    "chat_dir = os.path.join(path, \"dataset\")\n",
    "sender_indices = preprocess_data(chat_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing Data and Creating Vocabulary\n",
    "\n",
    "Now that we have preprocessed the data we can create our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tokenized_data, lines = create_vocab(chat_dir, sender_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = []\n",
    "for tokens, label in tokenized_data:\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    # the token that is not in vocab get assigned <unk>\n",
    "    indexed_data.append((indices, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    data = (lines[i], tokenized_data[i][0], indexed_data[i][0], indexed_data[i][1])\n",
    "    combined_data.append(data)\n",
    "\n",
    "\n",
    "dataset = ChatDataset(combined_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    assert isinstance(batch, list)\n",
    "    data = pad_sequence([b['data'] for b in batch])\n",
    "    lengths = torch.tensor([len(b['data']) for b in batch])\n",
    "    label = torch.stack([b['label'] for b in batch])\n",
    "    return {\n",
    "        'data': data,\n",
    "        'label': label,\n",
    "        'lengths': lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, collate_fn=collate)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        sampler=valid_sampler, collate_fn=collate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create and Train Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_accuracy(model, data_loader):\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, x in enumerate(data_loader):\n",
    "        input = x['data'].to(device)\n",
    "        lengths = x['lengths']\n",
    "        label = x['label'].to(device)\n",
    "        pred = model(input, lengths)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        corrects += torch.count_nonzero(torch.eq(pred, label))\n",
    "        total += label.numel()\n",
    "\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print('Step {} / {}'.format(i, len(data_loader)))\n",
    "\n",
    "    return corrects / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.path.join(path, \"model\", \"chat_model.p\")\n",
    "\n",
    "# model = torch.load(model_save_path)\n",
    "model = RNNClassifier(len(vocab), 75, 32, 10, num_layers=1)\n",
    "\n",
    "# Move model to the device we are using\n",
    "model = model.to(device)\n",
    "gclip = 10\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, loss_func, epochs=10):\n",
    "    model.train()\n",
    "    for epoch_id in range(epochs):\n",
    "        for i, batch in enumerate(train_loader, 1):\n",
    "            data, labels, lengths = batch['data'].to(\n",
    "                device), batch['label'].to(device), batch['lengths'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, lengths)\n",
    "            outputs = outputs.to(device)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), gclip)\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch_id + 1} \\t loss: {loss.item()}')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t loss: 1.7110021114349365\n",
      "Epoch 2 \t loss: 1.5741403102874756\n",
      "Epoch 3 \t loss: 1.617937445640564\n",
      "Epoch 4 \t loss: 1.7969694137573242\n",
      "Epoch 5 \t loss: 2.114994764328003\n",
      "Epoch 6 \t loss: 1.7234978675842285\n",
      "Epoch 7 \t loss: 2.0141217708587646\n",
      "Epoch 8 \t loss: 1.7327349185943604\n",
      "Epoch 9 \t loss: 2.35719895362854\n",
      "Epoch 10 \t loss: 1.8483942747116089\n",
      "Epoch 11 \t loss: 1.6028711795806885\n",
      "Epoch 12 \t loss: 1.6984831094741821\n",
      "Epoch 13 \t loss: 1.8550447225570679\n",
      "Epoch 14 \t loss: 1.9911614656448364\n",
      "Epoch 15 \t loss: 1.4938912391662598\n",
      "Epoch 16 \t loss: 2.1137077808380127\n",
      "Epoch 17 \t loss: 1.4977675676345825\n",
      "Epoch 18 \t loss: 2.008235216140747\n",
      "Epoch 19 \t loss: 1.6886136531829834\n",
      "Epoch 20 \t loss: 1.1570345163345337\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_loader, val_loader, loss_func, epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 / 4995\n",
      "Step 200 / 4995\n",
      "Step 300 / 4995\n",
      "Step 400 / 4995\n",
      "Step 500 / 4995\n",
      "Step 600 / 4995\n",
      "Step 700 / 4995\n",
      "Step 800 / 4995\n",
      "Step 900 / 4995\n",
      "Step 1000 / 4995\n",
      "Step 1100 / 4995\n",
      "Step 1200 / 4995\n",
      "Step 1300 / 4995\n",
      "Step 1400 / 4995\n",
      "Step 1500 / 4995\n",
      "Step 1600 / 4995\n",
      "Step 1700 / 4995\n",
      "Step 1800 / 4995\n",
      "Step 1900 / 4995\n",
      "Step 2000 / 4995\n",
      "Step 2100 / 4995\n",
      "Step 2200 / 4995\n",
      "Step 2300 / 4995\n",
      "Step 2400 / 4995\n",
      "Step 2500 / 4995\n",
      "Step 2600 / 4995\n",
      "Step 2700 / 4995\n",
      "Step 2800 / 4995\n",
      "Step 2900 / 4995\n",
      "Step 3000 / 4995\n",
      "Step 3100 / 4995\n",
      "Step 3200 / 4995\n",
      "Step 3300 / 4995\n",
      "Step 3400 / 4995\n",
      "Step 3500 / 4995\n",
      "Step 3600 / 4995\n",
      "Step 3700 / 4995\n",
      "Step 3800 / 4995\n",
      "Step 3900 / 4995\n",
      "Step 4000 / 4995\n",
      "Step 4100 / 4995\n",
      "Step 4200 / 4995\n",
      "Step 4300 / 4995\n",
      "Step 4400 / 4995\n",
      "Step 4500 / 4995\n",
      "Step 4600 / 4995\n",
      "Step 4700 / 4995\n",
      "Step 4800 / 4995\n",
      "Step 4900 / 4995\n",
      "accuracy on test set: 0.3561703860759735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"accuracy on test set: {}\".format(compute_accuracy(model, val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model, model_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.chat_dataset import tokenize\n",
    "from time import sleep\n",
    "model.eval()\n",
    "\n",
    "pred_indices = {value:key for (key, value) in sender_indices.items()}\n",
    "\n",
    "text = input(\"Enter text: \")\n",
    "tokens = tokenize(text.lower())\n",
    "indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "sequence = torch.tensor([indices]).permute(1,0).to(device)\n",
    "pred = model.predict(sequence)\n",
    "print(f'{pred_indices[pred.item()]}: {text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c00457d27ce730ef0cdcf7f4aede23c3643b99c873f2bc6551ffd637bf1447c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
